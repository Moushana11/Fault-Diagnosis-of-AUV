{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "365eae93-5a34-458e-aeee-9207de9481db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9cef27-9c69-43a8-840b-e3115ef9d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1764d972-6325-4d4a-aaa8-562d6489e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder_path = r'C:\\Fault-Diagnosis-of-AUV\\Dataset\\Dataset' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "740e578e-b32f-45a2-8c82-d750ec1b1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_types = ['train', 'test']\n",
    "fault_types = {\n",
    "    'AddWeight': 'load_increase',\n",
    "    'Normal': 'normal_state',\n",
    "    'PressureGain_constant': 'depth_sensor_failure',\n",
    "    'PropellerDamage_bad': 'severe_propeller_damage',\n",
    "    'PropellerDamage_slight': 'slight_propeller_damage'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44225d57-da36-4adb-8d3b-d984fb44f1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Test Data:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "combined_data = {'train': [], 'test': []}\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    dataset_path = os.path.join(base_folder_path, dataset_type)\n",
    "    for fault_folder, fault_label in fault_types.items():\n",
    "        fault_path = os.path.join(dataset_path, fault_folder)\n",
    "        if os.path.exists(fault_path):\n",
    "            for file_name in os.listdir(fault_path):\n",
    "                if file_name.endswith('.csv'):\n",
    "                    file_path = os.path.join(fault_path, file_name)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        df['Fault_Type'] = fault_label\n",
    "                        df['Dataset_Type'] = dataset_type\n",
    "                        combined_data[dataset_type].append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame for each dataset type\n",
    "train_df = pd.concat(combined_data['train'], ignore_index=True) if combined_data['train'] else pd.DataFrame()\n",
    "test_df = pd.concat(combined_data['test'], ignore_index=True) if combined_data['test'] else pd.DataFrame()\n",
    "\n",
    "# Display the first few rows of the combined DataFrames\n",
    "print(\"Training Data:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b73ce8-8e0c-4122-8d1d-3bbc8e5e5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and test datasets\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc7b0999-a62c-4d77-ad2b-0faa730d3397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7334c44-b265-43df-9993-0bbb3c73e37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cbcef01-a294-41ef-881b-301bd4d99ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb01023b-8902-4a53-ba21-b551d877c27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "facdfe9b-4d3a-46d1-8cb2-2f0ce1701cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=0, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebb21ba0-8034-4e9f-98f1-a93a89487037",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot describe a DataFrame without columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m combined_df\u001b[38;5;241m.\u001b[39mdescribe()\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mE:\\anac\\Lib\\site-packages\\pandas\\core\\generic.py:11976\u001b[0m, in \u001b[0;36mNDFrame.describe\u001b[1;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[0;32m  11734\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m  11735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdescribe\u001b[39m(\n\u001b[0;32m  11736\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11739\u001b[0m     exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  11740\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m  11741\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  11742\u001b[0m \u001b[38;5;124;03m    Generate descriptive statistics.\u001b[39;00m\n\u001b[0;32m  11743\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11974\u001b[0m \u001b[38;5;124;03m    max            NaN      3.0\u001b[39;00m\n\u001b[0;32m  11975\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 11976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m describe_ndframe(\n\u001b[0;32m  11977\u001b[0m         obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  11978\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m  11979\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m  11980\u001b[0m         percentiles\u001b[38;5;241m=\u001b[39mpercentiles,\n\u001b[0;32m  11981\u001b[0m     )\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescribe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mE:\\anac\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:91\u001b[0m, in \u001b[0;36mdescribe_ndframe\u001b[1;34m(obj, include, exclude, percentiles)\u001b[0m\n\u001b[0;32m     87\u001b[0m     describer \u001b[38;5;241m=\u001b[39m SeriesDescriber(\n\u001b[0;32m     88\u001b[0m         obj\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj),\n\u001b[0;32m     89\u001b[0m     )\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     describer \u001b[38;5;241m=\u001b[39m DataFrameDescriber(\n\u001b[0;32m     92\u001b[0m         obj\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj),\n\u001b[0;32m     93\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m     94\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     95\u001b[0m     )\n\u001b[0;32m     97\u001b[0m result \u001b[38;5;241m=\u001b[39m describer\u001b[38;5;241m.\u001b[39mdescribe(percentiles\u001b[38;5;241m=\u001b[39mpercentiles)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(NDFrameT, result)\n",
      "File \u001b[1;32mE:\\anac\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:162\u001b[0m, in \u001b[0;36mDataFrameDescriber.__init__\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude \u001b[38;5;241m=\u001b[39m exclude\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot describe a DataFrame without columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(obj)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot describe a DataFrame without columns"
     ]
    }
   ],
   "source": [
    "combined_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2322c7-6614-424a-b4e5-4b76f11aadb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numerical columns for correlation calculation\n",
    "num_columns = ['time', 'pwm1', 'pwm2', 'pwm3', 'pwm4', 'depth', 'press', 'voltage',\n",
    "                     'roll', 'pitch', 'yaw', 'a_x', 'a_y', 'a_z', 'w_row', 'w_pitch', 'w_yaw']\n",
    "correlation_matrix = combined_df[num_columns].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb78d7f-bc34-4853-8023-aefdde1867be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(16, 17))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, linewidths=.5)\n",
    "\n",
    "# Set the title for the heatmap\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1447e8-d449-492b-9350-9dde0c2d6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"Fault_Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764d29d-9307-4e42-b370-1741ca5706db",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"Fault_Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d10f8a-2f7d-4c26-84dd-7417020ab8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e76df1-ae8c-43a8-a7ee-aeba1d4bfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47f5c1-214b-42df-8cba-b1397939f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58011cc-f3e5-424c-8355-7dc9135b7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"Fault_Type\"] = label_encoder.fit_transform(combined_df[\"Fault_Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a0330-a838-4e6c-aa5c-c64e13efb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1de6b-7ac8-4bbc-a5f0-a25f45ed3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistical summary of the DataFrame\n",
    "print(combined_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff73f5-d5be-431d-bcca-137dcf4d8222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(combined_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0fbef7-ed31-469d-85f2-bb5185f996ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histograms for each feature\n",
    "combined_df.hist(figsize=(15, 15))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de078f9-1420-4cb1-88e9-c65dff05f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"Fault_Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd268bbc-8a1d-4dd8-972d-a84bb9ae73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"Fault_Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46db69f-57b4-4776-8045-4ee079a7fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7b1ef-6689-4339-9d10-4f47960b1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['label'] = combined_df['voltage'].apply(lambda x: \"fault\" if x > 12.06 else \"normal\")\n",
    "\n",
    "# Display the first few rows with the new label\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a23c2-5d82-4efb-bdd9-2dfa5654044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "print(combined_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db2d7d-7dcd-4c88-a194-56fad40907fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last few rows of the DataFrame\n",
    "print(combined_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a9cfb-4a8f-421d-bfd6-57cbe3d9a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the DataFrame\n",
    "print(combined_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa27622-d40f-409f-a5b2-e340246c398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(combined_df.drop(columns=['label']))\n",
    "\n",
    "# Create a DataFrame with scaled features\n",
    "scaled_df = pd.DataFrame(scaled_features, index=combined_df.index, columns=combined_df.columns[:-1])\n",
    "scaled_df['label'] = combined_df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1b091-c844-4d63-9d71-31f4d9c3628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the folder containing the datasets\n",
    "folder_path = r'E:\\Fault-Diagnosis-of-AUV\\Dataset\\Dataset'   # Update this path to your folder\n",
    "\n",
    "# Define the labeling function based on specific criteria\n",
    "def label_data(row):\n",
    "    if row['voltage'] < 12.0:  # Update this condition as needed\n",
    "        return 'fault'\n",
    "    else:\n",
    "        return 'normal'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            # Load the dataset\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if 'voltage' column exists\n",
    "            if 'voltage' not in data.columns:\n",
    "                print(f\"Skipping {filename}: 'voltage' column not found\")\n",
    "                continue\n",
    "            \n",
    "            # Apply the labeling function to each row in the dataset\n",
    "            data['label'] = data.apply(label_data, axis=1)\n",
    "            \n",
    "            # Print the labeled data\n",
    "            print(f\"Labeled data for {filename}:\")\n",
    "            print(data.head())  # Adjust the number of rows to print as needed\n",
    "            \n",
    "            # Define the path to save the labeled dataset\n",
    "            labeled_file_path = os.path.join(folder_path, f'labeled_{filename}')\n",
    "            \n",
    "            # Save the labeled dataset to a new CSV file\n",
    "            data.to_csv(labeled_file_path, index=False)\n",
    "            \n",
    "            print(f\"Labeled dataset saved to: {labeled_file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7589f2c-918c-4cd4-a5f5-2e0ebbbd0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(row):\n",
    "    if row['voltage'] < 12.0:  # Update this condition as needed\n",
    "        return 'fault'\n",
    "    else:\n",
    "        return 'normal'\n",
    "\n",
    "# Loop through each file in the folder for labeling\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            # Load the dataset\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if 'voltage' column exists\n",
    "            if 'voltage' not in data.columns:\n",
    "                print(f\"Skipping {filename}: 'voltage' column not found\")\n",
    "                continue\n",
    "            \n",
    "            # Apply the labeling function to each row in the dataset\n",
    "            data['label'] = data.apply(label_data, axis=1)\n",
    "            \n",
    "            # Print the labeled data\n",
    "            print(f\"Labeled data for {filename}:\")\n",
    "            print(data.head())  # Adjust the number of rows to print as needed\n",
    "            \n",
    "            # Define the path to save the labeled dataset\n",
    "            labeled_file_path = os.path.join(folder_path, f'labeled_{filename}')\n",
    "            \n",
    "            # Save the labeled dataset to a new CSV file\n",
    "            data.to_csv(labeled_file_path, index=False)\n",
    "            \n",
    "            print(f\"Labeled dataset saved to: {labeled_file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd526be-6bc9-4053-b72c-f87ba8ba3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the path where your datasets are stored\n",
    "input_folder = 'E:\\\\Fault-Diagnosis-of-AUV\\\\Dataset\\\\Dataset'\n",
    "output_folder = 'E:\\\\Fault-Diagnosis-of-AUV\\\\Dataset\\\\Dataset\\\\cleaned_labeled_train'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "file_list = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Function to clean and label a dataset\n",
    "def clean_and_label_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Handle missing values (example: fill with mean)\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Convert data types if necessary\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['time']):\n",
    "        df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "\n",
    "    # Handle outliers (example: capping values to 1.5*IQR)\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "    # Normalize/scale data if needed (example: standard scaling)\n",
    "    scaler = StandardScaler()\n",
    "    df[df.columns.difference(['time', 'label'])] = scaler.fit_transform(df[df.columns.difference(['time', 'label'])])\n",
    "\n",
    "    # Add a label column based on depth\n",
    "    df['label'] = df['depth'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ee864-fd64-4ec7-bd92-af5ab1ab3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the path where your datasets are stored\n",
    "input_folder = 'E:\\\\Fault-Diagnosis-of-AUV\\\\Dataset\\\\Dataset'\n",
    "output_folder = 'E:\\\\Fault-Diagnosis-of-AUV\\\\Dataset\\\\Dataset\\\\cleaned_labeled_train'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "file_list = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Function to clean and label a dataset\n",
    "def clean_labeled_train(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "    # Handle missing values (example: fill with mean)\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Convert data types if necessary\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['time']):\n",
    "        df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "\n",
    "    # Handle outliers (example: capping values to 1.5*IQR)\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "    # Normalize/scale data if needed (example: standard scaling)\n",
    "    scaler = StandardScaler()\n",
    "    columns_to_scale = df.columns.difference(['time', 'label'])\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "    # Add a label column based on depth\n",
    "    df['label'] = df['depth'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Process each file in the input folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    cleaned_df = clean_labeled_train(file_path)\n",
    "    \n",
    "    # Save the cleaned and labeled dataframe to the output folder\n",
    "    output_file_path = os.path.join(output_folder, file_name)\n",
    "    cleaned_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Saved cleaned data to: {output_file_path}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b434b-c2a5-4f03-8656-5bdf38bbb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the path where your datasets are stored\n",
    "input_folder = 'E:\\\\Fault-Diagnosis-of-AUV\\\\Dataset\\\\Dataset'\n",
    "output_folder = 'E:\\\\Fault-Diagnosis-of-AUV\\\\Dataset\\\\Dataset\\\\cleaned_labeled_train'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "file_list = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Function to clean and label a dataset\n",
    "def clean_and_label_dataset(file_path):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"File {file_path} is empty. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Initial data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Handle missing values (example: fill with mean)\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Convert data types if necessary\n",
    "    if 'time' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['time']):\n",
    "        df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "\n",
    "    print(\"Data after handling missing values and duplicates:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Handle outliers (example: capping values to 1.5*IQR)\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "    print(\"Data after handling outliers:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Normalize/scale data if needed (example: standard scaling)\n",
    "    scaler = StandardScaler()\n",
    "    columns_to_scale = df.columns.difference(['time', 'label'])\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "    print(\"Data after scaling:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Add a label column based on depth\n",
    "    if 'depth' in df.columns:\n",
    "        df['label'] = df['depth'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "    else:\n",
    "        print(\"Depth column not found. Skipping labeling.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Data after adding label column:\")\n",
    "    print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Process each file in the input folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    cleaned_df = clean_and_label_dataset(file_path)\n",
    "    \n",
    "    if cleaned_df is not None:\n",
    "        # Save the cleaned and labeled dataframe to the output folder\n",
    "        output_file_path = os.path.join(output_folder, file_name)\n",
    "        try:\n",
    "            cleaned_df.to_csv(output_file_path, index=False)\n",
    "            print(f\"Saved cleaned data to: {output_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving file {output_file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping file: {file_path}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9623c35-3cc1-45da-9df4-21f8e1554b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to clean and label a dataset\n",
    "def clean_and_label_dataset(df):\n",
    "    # Handle missing values (example: fill with mean)\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Convert data types if necessary\n",
    "    if 'time' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['time']):\n",
    "        df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "\n",
    "    # Handle outliers (example: capping values to 1.5*IQR)\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "    # Normalize/scale data if needed (example: standard scaling)\n",
    "    scaler = StandardScaler()\n",
    "    columns_to_scale = df.columns.difference(['time', 'label'])\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "    # Add a label column based on depth\n",
    "    if 'depth' in df.columns:\n",
    "        df['label'] = df['depth'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "    else:\n",
    "        print(\"Depth column not found. Skipping labeling.\")\n",
    "        return None\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define the path where your datasets are stored\n",
    "input_folder = 'E:\\\\Fault-Diagnosis-of-AUV\\\\Dataset\\\\Dataset'\n",
    "output_folder = 'E:\\\\Fault-Diagnosis-of-AUV\\\\Dataset\\\\Dataset\\\\cleaned_labeled_train'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "file_list = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Process each file in the input folder\n",
    "all_data = pd.DataFrame()\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        cleaned_df = clean_and_label_dataset(df)\n",
    "        if cleaned_df is not None:\n",
    "            output_file_path = os.path.join(output_folder, file_name)\n",
    "            cleaned_df.to_csv(output_file_path, index=False)\n",
    "            print(f\"Saved cleaned data to: {output_file_path}\")\n",
    "            all_data = pd.concat([all_data, cleaned_df], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "print(\"Processing complete.\")\n",
    "\n",
    "# Display the cleaned and labeled dataset\n",
    "if not all_data.empty:\n",
    "    display(all_data.head())\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = all_data.corr()\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(16, 17))\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, linewidths=.5)\n",
    "\n",
    "    # Set the title for the heatmap\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21564c28-7d38-4246-ac16-548517c9f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the path where your datasets are stored\n",
    "input_folder = 'E:\\\\Fault-Diagnosis-of-AUV\\\\Dataset\\\\Dataset'\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "file_list = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Function to label a dataset based on depth\n",
    "def label_dataset(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Add a label column based on depth\n",
    "        if 'depth' in df.columns:\n",
    "            df['label'] = df['depth'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"Depth column not found in file {file_path}. Skipping labeling.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or labeling file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process each file in the input folder\n",
    "all_data = pd.DataFrame()\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    print(f\"Labeling dataset for file: {file_path}\")\n",
    "    labeled_df = label_dataset(file_path)\n",
    "    if labeled_df is not None:\n",
    "        all_data = pd.concat([all_data, labeled_df], ignore_index=True)\n",
    "\n",
    "# Display the labeled dataset\n",
    "if not all_data.empty:\n",
    "    display(all_data.head())\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = all_data.corr()\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(16, 17))\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, linewidths=.5)\n",
    "\n",
    "    # Set the title for the heatmap\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available to display.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368de23-0574-468c-823d-09e424f479f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['label'] = combined_df['voltage'].apply(lambda x: \"fault\" if x > 12.06 else \"normal\")\n",
    "\n",
    "# Display the first few rows with the new label\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49432b-6bbb-4bf1-8022-f1c201e68bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadad9a3-8c49-4609-9b0f-e7310bf7b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e526fa-cc36-4f68-aff8-358322302182",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ced6e0-f23c-42ab-9c5e-0c79aab6b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c0055-0252-4464-8d52-0eeffaaf7de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
